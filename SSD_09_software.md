
9.1. What is software?

The year is 1925. Ronald Fisher is a geneticist and statistician working at Rothamsted Experimental Station, an agricultural research institute located in the English countryside. A plethora of long term experiments give Fisher a bumper crop of data to analyze. However, though the quantity of data is high, sample sizes are low. One study, examining the effects of rainfall on the growth of wheat, incorporates data from just thirteen plots of land.

Concerned with the generalizability of his work, Fisher synthesizes several recent advances in “small sample statistics” into a framework known as “significance testing”. He expands the utility of Student’s t-test - a statistical device initially developed by William Sealy Gosset to monitor the quality of beer - and develops a complementary test known as the Analysis of Variance (ANOVA). To ensure these innovations are accessible to the research community beyond Rothamsted, Fisher publishes an influential volume Statistical Methods for Research Workers.

Central to the book, and significance testing more generally, is the null hypothesis - the position that there is no significant difference between groups of data. The results of statistical tests like t-tests and ANOVAs indicate the likelihood of observing a pattern of results when the null hypothesis is true. In quantitative terms, this likelihood is expressed as a p-value. In Statistical Methods for Research Workers, Fisher introduces an informal criterion for rejecting the null hypothesis: p < 0.05. These developments only grow in influence with the publication of The Design of Experiments.

Among the details often neglected from this account of the early history of statistical research methods is the degree to which it intersects with the development of modern computing. At Rothamsted, Fisher made extensive use of “The Millionaire”, the first commercially successful calculating machine. In the subsequent decades, work at Rothamsted would continue to leverage advances in computational technology. By the 1960s, researchers at the institute had developed thousands of miles of paper tape - for calculating regressions, multivariate analyses, and conducting all manner of other statistical operations. In modern parlance, we would call this software.

By 1968, the software at Rothamsted was compiled into the statistical software package Genstat. Nearly simultaneously, teams at the North Carolina State University and University of Chicago developed SAS and SPSS. Here at Stanford, researchers continue to use these tools - and so, so many others - as part of their work. Software, like data, is also an important product of the research process in and of itself. 

Especially under our workflow-based definition, software can be categorized as a form of data. Software can be the tool through which data is collected, recorded, and analyzed. It is also - quite literally - the precise steps implemented during part of the research process. As such, software is an essential element of establishing data provenance. We will get into the how later in this chapter, but, like data, research-related software needs to be thoughtfully managed and disseminated.

But software is also very different from data. This is a tremendous oversimplification but data is while software does. A set of research data is not necessarily a static object but, in general, data is something we act upon while software performs a particular function (e.g. analyzing a dataset). Software is executable. Software also has a different set of needs than data and its proper management requires different skills and vocabulary.

9.2. What is your software?

9.3. Talking about and sharing software
